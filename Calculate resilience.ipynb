{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from scipy.signal import savgol_filter\n",
    "import itertools\n",
    "import os\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import scipy.stats, scipy.signal\n",
    "import numpy.ma as ma\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import datetime\n",
    "import warnings\n",
    "import rasterio\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a37b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pre-processing modified from the method of Chen et al. (2004): https://www.sciencedirect.com/science/article/abs/pii/S003442570400080X\n",
    "def clean_ndvi_timeseries(orig_input):\n",
    "    \"\"\"Following Chen et al. 2004\"\"\"\n",
    "    # Remove impossible values, linear fit\n",
    "    roll_ma = orig_input.rolling(window='20D').apply(np.nanmax)\n",
    "    roll_mi = orig_input.rolling(window='20D').apply(np.nanmin)\n",
    "\n",
    "    diff = roll_ma - roll_mi\n",
    "    orig_input[diff > 0.4] = np.nan\n",
    "\n",
    "    # Remove remaining missed clouds/water\n",
    "    orig_input[orig_input < 0] = np.nan\n",
    "\n",
    "    # Mask out non-vegetated areas\n",
    "    mns = []\n",
    "    for yr in orig_input.groupby(orig_input.index.year):\n",
    "        yrdata = yr[1]\n",
    "        mns.append(np.nanmean(yrdata.values))\n",
    "\n",
    "    if np.nanmean(mns) < 0.1:\n",
    "        return np.array((np.nan,) * orig_input.shape[0]), False\n",
    "\n",
    "    # STEP 1 - Do the interpolation\n",
    "    else:\n",
    "        data = orig_input.interpolate('linear').bfill().ffill()\n",
    "        N_0 = data.copy()\n",
    "\n",
    "        # STEP 2 - Fit SG Filter, using best fitting parameters\n",
    "        arr = np.empty((4, 3))\n",
    "        for m, d in list(itertools.product([4, 5, 6, 7], [2, 3, 4])):\n",
    "            data_smoothed = pd.Series(savgol_filter(data.values, window_length=2 * m + 1, polyorder=d, deriv=0, delta=1.0), index=data.index)\n",
    "            err = np.nansum(data_smoothed - data) ** 2\n",
    "            arr[m - 4, d - 2] = err\n",
    "        m, d = np.where(arr == np.nanmin(arr))\n",
    "        m, d = m[0] + 4, d[0] + 2\n",
    "        data_smoothed = pd.Series(savgol_filter(data.values, window_length=2 * m + 1, polyorder=d, deriv=0, delta=1.0), index=data.index)\n",
    "        diff = N_0 - data_smoothed\n",
    "\n",
    "        # STEP 3 - Create weights array based on difference from curve\n",
    "        max_dif = np.nanmax(np.abs(diff.values))\n",
    "        weights = np.zeros(np.array(data_smoothed.values).shape)\n",
    "        weights[data_smoothed >= N_0] = 1\n",
    "        weights[data_smoothed < N_0] = 1 - (np.abs(diff.values) / max_dif)[data_smoothed < N_0]\n",
    "\n",
    "        # STEP 4 - Replace values that were smoothed downwards with original values\n",
    "        data_smoothed[N_0 >= data_smoothed] = N_0[N_0 >= data_smoothed]\n",
    "        data_smoothed[N_0 < data_smoothed] = data_smoothed[N_0 < data_smoothed]\n",
    "\n",
    "        # STEP 5 - Resmooth with different parameters\n",
    "        data_fixed = pd.Series(savgol_filter(data.values, window_length=9, polyorder=6, deriv=0, delta=1.0), index=data.index)\n",
    "\n",
    "        # STEP 6 - Calculate the fitting effect\n",
    "        Fe_0 = np.nansum(np.abs(data_fixed.values - N_0.values) * weights)\n",
    "\n",
    "        data = data_fixed.copy()\n",
    "\n",
    "        count = 0\n",
    "        # while Fe_0 >= Fe and Fe <= Fe_1:\n",
    "        # while Fe_0 >= Fe:\n",
    "        while count < 5:  # Faster and just as effective to limit the number of loops rather than having a convergence factor\n",
    "            # STEP 4 - Replace values that were smoothed downwards with original values\n",
    "            data[N_0 >= data] = N_0[N_0 >= data]\n",
    "            data[N_0 < data] = data[N_0 < data]\n",
    "\n",
    "            # STEP 5 - Resmooth with different parameters\n",
    "            data_fixed = pd.Series(savgol_filter(data.values, window_length=9, polyorder=6, deriv=0, delta=1.0), index=data.index)\n",
    "            # data_fixed = pd.Series(savitzky_golay(data.values, 5, 2, deriv=0, rate=1), index=data.index)\n",
    "\n",
    "            # STEP 6 - Calculate the fitting effect\n",
    "            # Fe = np.nansum(np.abs(data_fixed.values - N_0.values) * weights)\n",
    "            # data = data_fixed.copy()\n",
    "            # Fe_0 = Fe.copy()\n",
    "            count += 1\n",
    "            # if Fe <= Fe_0:\n",
    "            # data_fixed = data_fixed_smoothed.copy()\n",
    "            # data_fixed[N_0 >= data_fixed_smoothed] = N_0[N_0 >= data_fixed_smoothed]\n",
    "            # data_fixed[N_0 < data_fixed_smoothed] = data_fixed_smoothed[N_0 < data_fixed_smoothed]\n",
    "            # if Fe > Fe_0:\n",
    "            #    break\n",
    "        return np.array(data.values), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c7142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Detrend and Deseason\n",
    "def harmonic_fit(ser, order=3):\n",
    "    import statsmodels.api as sm\n",
    "    harm_freq = list(range(1, order + 1))\n",
    "\n",
    "    x, y = ser.index, ser.values\n",
    "    x = pd.to_datetime(x, format='%Y%m%d')\n",
    "    x = np.array([(x - pd.Timestamp('2000-01-01')).days for x in x]) / 365.25\n",
    "    x_rad = x * 2 * np.pi  # Convert days to radians for harmonic fitting\n",
    "\n",
    "    # Create empty array to hold the independents\n",
    "    nr_indep = order * 2 + 2\n",
    "    indep = np.empty((y.shape[0], nr_indep))\n",
    "\n",
    "    # Add constant for intercept and then time\n",
    "    indep[:, 0] = 1\n",
    "    indep[:, 1] = x_rad\n",
    "\n",
    "    # Now create the harmonic variables\n",
    "    i = 2\n",
    "    for freq in harm_freq:\n",
    "        cos = np.cos(x_rad * freq)\n",
    "        sin = np.sin(x_rad * freq)\n",
    "        indep[:, i] = cos\n",
    "        i = i + 1\n",
    "        indep[:, i] = sin\n",
    "        i = i + 1\n",
    "\n",
    "    model = sm.OLS(y, indep, missing='drop').fit()\n",
    "    coefs = model.params\n",
    "    fitted = []\n",
    "    for t in range(x_rad.shape[0]):\n",
    "        data = indep[t, :]\n",
    "        harm_term = np.nansum(coefs * data)\n",
    "        fitted.append(harm_term)\n",
    "    fitted = np.array(fitted)\n",
    "    return pd.Series(ser.values - fitted, index=ser.index)\n",
    "\n",
    "\n",
    "def runmean(x, w):\n",
    "    n = x.shape[0]\n",
    "    xs = np.zeros_like(x)\n",
    "    for i in range(w // 2):\n",
    "        xs[i] = np.nanmean(x[: i + w // 2 + 1])\n",
    "    for i in range(n - w // 2, n):\n",
    "        xs[i] = np.nanmean(x[i - w // 2 + 1:])\n",
    "    for i in range(w // 2, n - w // 2):\n",
    "        xs[i] = np.nanmean(x[i - w // 2: i + w // 2 + 1])\n",
    "    return x - xs\n",
    "\n",
    "\n",
    "def deseason_detrend(ser, yrs, yl):\n",
    "    rm_offline = pd.Series(runmean(ser.values, yrs*yl), index=ser.index)\n",
    "    deseason_rolling = harmonic_fit(rm_offline, order=3)\n",
    "    return deseason_rolling.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cc464",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cauculate theoretical recovery rates from AC1/variance\n",
    "def calc_ar1(x):\n",
    "    # return np.corrcoef(x[:-1], x[1:])[0,1]\n",
    "    return ma.corrcoef(ma.masked_invalid(x[:-1]), ma.masked_invalid(x[1:]))[0, 1]\n",
    "\n",
    "\n",
    "def compute_lam(x, dt):\n",
    "    dx = (x[1:] - x[:-1]) / dt\n",
    "    return scipy.stats.linregress(x[:-1], dx)[0]\n",
    "\n",
    "\n",
    "def compute_sigma(x, dt=1):\n",
    "    dx = (x[1:] - x[:-1]) / dt\n",
    "    lamb = compute_lam(x, dt)\n",
    "    diff = dx - lamb * x[:-1]\n",
    "    return np.std(diff) * np.sqrt(dt)\n",
    "\n",
    "\n",
    "def check_ts(x):\n",
    "    v = np.nanvar(x)\n",
    "    ar1 = calc_ar1(x)\n",
    "    lambda_ar1 = np.log(ar1)\n",
    "    sigma = compute_sigma(x)\n",
    "    lambda_var = -sigma ** 2 / (2 * v)\n",
    "\n",
    "    return lambda_ar1, lambda_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51509ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
